{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-language2label trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/notebooks/language2motion.gt/code\")\n",
      "\t\tBatcher\n",
      "\t\tModelSupport\n",
      "\t\tDatasets\n",
      "\t\tTextModels\n",
      "With SwiftPM flags: ['-c', 'release']\n",
      "Working in: /tmp/tmprgyxdz3f/swift-install\n",
      "[1/5] Compiling STBImage stb_image_write.c\n",
      "[2/5] Compiling Batcher Backend.swift\n",
      "[3/5] Compiling STBImage stb_image.c\n",
      "[4/5] Compiling SwiftProtobuf AnyMessageStorage.swift\n",
      "/notebooks/language2motion.gt/swift-install/package/.build/checkouts/swift-protobuf/Sources/SwiftProtobuf/BinaryDelimited.swift:198:7: warning: variable 'readBuffer' was never mutated; consider changing to 'let' constant\n",
      "  var readBuffer = UnsafeMutablePointer<UInt8>.allocate(capacity: 1)\n",
      "  ~~~ ^\n",
      "  let\n",
      "[5/6] Compiling ModelSupport BijectiveDictionary.swift\n",
      "[6/7] Compiling Datasets CIFAR10.swift\n",
      "[7/8] Compiling TextModels Attention.swift\n",
      "[8/9] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "[9/9] Linking libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location /notebooks/language2motion.gt/swift-install\n",
    "%install-swiftpm-flags -c release\n",
    "%install '.package(path: \"/notebooks/language2motion.gt/code\")' Batcher ModelSupport Datasets TextModels\n",
    "\n",
    "import Datasets\n",
    "import Foundation\n",
    "import ModelSupport\n",
    "import TensorFlow\n",
    "import TextModels\n",
    "import PythonKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT pre-trained model 'BERT Base Uncased'.\n",
      "Loading resource: uncased_L-12_H-768_A-12\n"
     ]
    }
   ],
   "source": [
    "let bertPretrained = BERT.PreTrainedModel.bertBase(cased: false, multilingual: false)\n",
    "let workspaceURL = URL(\n",
    "    fileURLWithPath: \"bert_models\", isDirectory: true,\n",
    "    relativeTo: URL(\n",
    "        fileURLWithPath: NSTemporaryDirectory(),\n",
    "        isDirectory: true))\n",
    "let bert = try BERT.PreTrainedModel.load(bertPretrained)(from: workspaceURL)\n",
    "var bertClassifier = BERTClassifier(bert: bert, classCount: 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"/notebooks/language2motion.gt/code/Sources/BERT-language2label/Language2Label.swift\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset acquired.\n"
     ]
    }
   ],
   "source": [
    "let maxSequenceLength = 30\n",
    "let batchSize = 2048\n",
    "\n",
    "let dsURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/labels_ds_v2_manual_labels.csv\")\n",
    "\n",
    "var dataset = try Language2Label(\n",
    "  datasetURL: dsURL,\n",
    "  maxSequenceLength: maxSequenceLength,\n",
    "  batchSize: batchSize,\n",
    "  entropy: SystemRandomNumberGenerator()\n",
    ") { (example: Language2LabelExample) -> LabeledTextBatch in\n",
    "  let textBatch = bertClassifier.bert.preprocess(\n",
    "    sequences: [example.text],\n",
    "    maxSequenceLength: maxSequenceLength)\n",
    "   return (data: textBatch, \n",
    "           label: example.label.map { \n",
    "               (label: Language2LabelExample.LabelTuple) in Tensor(Int32(label.idx))\n",
    "           }!\n",
    "          )\n",
    "}\n",
    "\n",
    "print(\"Dataset acquired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2409\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.trainingExamples.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 2 elements\n",
       "  ▿ data : TextBatch\n",
       "    - tokenIds : [[ 101, 1037, 2711, 2003, 5645, 1012,  102]]\n",
       "    - tokenTypeIds : [[0, 0, 0, 0, 0, 0, 0]]\n",
       "    - mask : [[1, 1, 1, 1, 1, 1, 1]]\n",
       "  - label : 0\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.trainingExamples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 2 elements\n",
       "  ▿ data : TextBatch\n",
       "    - tokenIds : [[  101,  1037,  2711,  7365,  1999,  1037,  4418,  4675, 20464,  7432, 14244,   102]]\n",
       "    - tokenTypeIds : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
       "    - mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
       "  - label : 1\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.validationExamples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "var optimizer = WeightDecayedAdam(\n",
    "    for: bertClassifier,\n",
    "    learningRate: LinearlyDecayedParameter(\n",
    "        baseParameter: LinearlyWarmedUpParameter(\n",
    "            baseParameter: FixedParameter<Float>(2e-5),\n",
    "            warmUpStepCount: 10,\n",
    "            warmUpOffset: 0),\n",
    "        slope: -5e-7,  // The LR decays linearly to zero in 100 steps.\n",
    "        startStep: 10),\n",
    "    weightDecayRate: 0.01,\n",
    "    maxGradientGlobalNorm: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT for the Language2Label task!\n",
      "[Epoch 1]\n",
      "epochBatches.count: 35\n",
      "  Training loss: 0.75371706\n",
      "  Training loss: 0.7014136\n",
      "  Training loss: 0.66437346\n",
      "  Training loss: 0.6026812\n",
      "  Training loss: 0.6085679\n",
      "  Training loss: 0.59096843\n",
      "  Training loss: 0.59411484\n",
      "  Training loss: 0.58522356\n",
      "  Training loss: 0.579469\n",
      "  Training loss: 0.56056\n",
      "  Training loss: 0.55604786\n",
      "  Training loss: 0.5380627\n",
      "  Training loss: 0.54076207\n",
      "  Training loss: 0.5436632\n",
      "  Training loss: 0.53137785\n",
      "  Training loss: 0.5426758\n",
      "  Training loss: 0.547717\n",
      "  Training loss: 0.5518289\n",
      "  Training loss: 0.5500601\n",
      "  Training loss: 0.5412386\n",
      "  Training loss: 0.5422206\n",
      "  Training loss: 0.54708993\n",
      "  Training loss: 0.5429764\n",
      "  Training loss: 0.5422126\n",
      "  Training loss: 0.53946966\n",
      "  Training loss: 0.53733534\n",
      "  Training loss: 0.5341239\n",
      "  Training loss: 0.53327227\n",
      "  Training loss: 0.5280953\n",
      "  Training loss: 0.5234589\n",
      "  Training loss: 0.5168105\n",
      "  Training loss: 0.51504934\n",
      "  Training loss: 0.5105922\n",
      "  Training loss: 0.51094896\n",
      "  Training loss: 0.50556344\n",
      "dataset.validationBatches.count: 9\n",
      "Accuracy: 524/603 (0.8689884) Eval loss: 0.37452942\n",
      "[Epoch 2]\n",
      "epochBatches.count: 35\n",
      "  Training loss: 0.32587007\n",
      "  Training loss: 0.33535346\n",
      "  Training loss: 0.30766222\n",
      "  Training loss: 0.3271141\n",
      "  Training loss: 0.32650635\n",
      "  Training loss: 0.322695\n",
      "  Training loss: 0.33194882\n",
      "  Training loss: 0.31945404\n",
      "  Training loss: 0.32456166\n",
      "  Training loss: 0.3321828\n",
      "  Training loss: 0.33306944\n",
      "  Training loss: 0.32393852\n",
      "  Training loss: 0.31477785\n",
      "  Training loss: 0.31848028\n",
      "  Training loss: 0.31636044\n",
      "  Training loss: 0.31856105\n",
      "  Training loss: 0.3201119\n",
      "  Training loss: 0.32662195\n",
      "  Training loss: 0.32999715\n",
      "  Training loss: 0.33760995\n",
      "  Training loss: 0.3358983\n",
      "  Training loss: 0.33804595\n",
      "  Training loss: 0.3402277\n",
      "  Training loss: 0.3420234\n",
      "  Training loss: 0.34233397\n",
      "  Training loss: 0.34142083\n",
      "  Training loss: 0.34403756\n",
      "  Training loss: 0.34910104\n",
      "  Training loss: 0.34657702\n",
      "  Training loss: 0.34776178\n",
      "  Training loss: 0.35055593\n",
      "  Training loss: 0.35238793\n",
      "  Training loss: 0.3506721\n",
      "  Training loss: 0.35069677\n",
      "  Training loss: 0.35335335\n",
      "dataset.validationBatches.count: 9\n",
      "Accuracy: 525/603 (0.8706468) Eval loss: 0.3807189\n",
      "[Epoch 3]\n",
      "epochBatches.count: 35\n",
      "  Training loss: 0.35572663\n",
      "  Training loss: 0.3393768\n",
      "  Training loss: 0.34366798\n",
      "  Training loss: 0.3585151\n",
      "  Training loss: 0.3609774\n",
      "  Training loss: 0.3688325\n",
      "  Training loss: 0.36198622\n",
      "  Training loss: 0.35937405\n",
      "  Training loss: 0.35432515\n",
      "  Training loss: 0.34502655\n",
      "  Training loss: 0.34410435\n",
      "  Training loss: 0.33933806\n",
      "  Training loss: 0.33830148\n",
      "  Training loss: 0.3373816\n",
      "  Training loss: 0.34186345\n",
      "  Training loss: 0.34608662\n",
      "  Training loss: 0.34314343\n",
      "  Training loss: 0.34889632\n",
      "  Training loss: 0.34600186\n",
      "  Training loss: 0.35088637\n",
      "  Training loss: 0.35260624\n",
      "  Training loss: 0.35492632\n",
      "  Training loss: 0.35523987\n",
      "  Training loss: 0.3503885\n",
      "  Training loss: 0.3528644\n",
      "  Training loss: 0.35344622\n",
      "  Training loss: 0.35356826\n",
      "  Training loss: 0.35125896\n",
      "  Training loss: 0.35300183\n",
      "  Training loss: 0.35356557\n",
      "  Training loss: 0.3538177\n",
      "  Training loss: 0.35196853\n",
      "  Training loss: 0.35183942\n",
      "  Training loss: 0.35258967\n",
      "  Training loss: 0.3511806\n",
      "dataset.validationBatches.count: 9\n",
      "Accuracy: 525/603 (0.8706468) Eval loss: 0.3807189\n"
     ]
    }
   ],
   "source": [
    "print(\"Training BERT for the Language2Label task!\")\n",
    "\n",
    "for (epoch, epochBatches) in dataset.trainingEpochs.prefix(3).enumerated() {\n",
    "    print(\"[Epoch \\(epoch + 1)]\")\n",
    "    Context.local.learningPhase = .training\n",
    "    var trainingLossSum: Float = 0\n",
    "    var trainingBatchCount = 0\n",
    "    print(\"epochBatches.count: \\(epochBatches.count)\")\n",
    "\n",
    "    for batch in epochBatches {\n",
    "        let (documents, labels) = (batch.data, Tensor<Int32>(batch.label))\n",
    "        let (loss, gradients) = valueWithGradient(at: bertClassifier) { model -> Tensor<Float> in\n",
    "            let logits = model(documents)\n",
    "            return softmaxCrossEntropy(logits: logits, labels: labels)\n",
    "        }\n",
    "\n",
    "        trainingLossSum += loss.scalarized()\n",
    "        trainingBatchCount += 1\n",
    "        optimizer.update(&bertClassifier, along: gradients)\n",
    "\n",
    "        print(\n",
    "            \"\"\"\n",
    "              Training loss: \\(trainingLossSum / Float(trainingBatchCount))\n",
    "            \"\"\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    print(\"dataset.validationBatches.count: \\(dataset.validationBatches.count)\")\n",
    "    Context.local.learningPhase = .inference\n",
    "    var devLossSum: Float = 0\n",
    "    var devBatchCount = 0\n",
    "    var correctGuessCount = 0\n",
    "    var totalGuessCount = 0\n",
    "\n",
    "    for batch in dataset.validationBatches {\n",
    "        let valBatchSize = batch.data.tokenIds.shape[0]\n",
    "\n",
    "        let (documents, labels) = (batch.data, Tensor<Int32>(batch.label))\n",
    "        let logits = bertClassifier(documents)\n",
    "        let loss = softmaxCrossEntropy(logits: logits, labels: labels)\n",
    "        devLossSum += loss.scalarized()\n",
    "        devBatchCount += 1\n",
    "\n",
    "        let correctPredictions = logits.argmax(squeezingAxis: 1) .== labels\n",
    "\n",
    "        correctGuessCount += Int(Tensor<Int32>(correctPredictions).sum().scalarized())\n",
    "        totalGuessCount += valBatchSize\n",
    "    }\n",
    "    \n",
    "    let accuracy = Float(correctGuessCount) / Float(totalGuessCount)\n",
    "    print(\n",
    "        \"\"\"\n",
    "        Accuracy: \\(correctGuessCount)/\\(totalGuessCount) (\\(accuracy)) \\\n",
    "        Eval loss: \\(devLossSum / Float(devBatchCount))\n",
    "        \"\"\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Prediction {\n",
    "    public let classIdx: Int\n",
    "    public let className: String\n",
    "    public let probability: Float\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: get num_best preds\n",
    "func predict(_ texts: [String], bertClassifier: BERTClassifier) -> [Prediction] {\n",
    "    print(\"predict()\")\n",
    "    print(\"texts: \\(texts.count)\")\n",
    "\n",
    "    let validationExamples = texts.map {\n",
    "        (text) -> TextBatch in\n",
    "        return bertClassifier.bert.preprocess(\n",
    "            sequences: [text],\n",
    "            maxSequenceLength: maxSequenceLength\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    print(\"validationExamples.count: \\(validationExamples.count)\")\n",
    "\n",
    "    print(\"batchSize: \\(batchSize)\")\n",
    "    print(\"maxSequenceLength: \\(maxSequenceLength)\")\n",
    "    print(\"batchSize / maxSequenceLength: \\(batchSize / maxSequenceLength)\")\n",
    "\n",
    "    let validationBatches = validationExamples.inBatches(of: batchSize / maxSequenceLength).map { \n",
    "        $0.paddedAndCollated(to: maxSequenceLength)\n",
    "    }\n",
    "    print(\"validationBatches: \\(validationBatches.count)\")\n",
    "    var preds: [Prediction] = []\n",
    "    for batch in validationBatches {\n",
    "        print(\"batch\")\n",
    "        let logits = bertClassifier(batch)\n",
    "        let probs = softmax(logits, alongAxis: 1)\n",
    "        let classIdxs = logits.argmax(squeezingAxis: 1)\n",
    "        let batchPreds = (0..<classIdxs.shape[0]).map { \n",
    "            (idx) -> Prediction in\n",
    "            let classIdx: Int = Int(classIdxs[idx].scalar!)\n",
    "            let prob = probs[idx, classIdx].scalar!\n",
    "            return Prediction(classIdx: classIdx, className: dataset.labels[classIdx], probability: prob)\n",
    "        }\n",
    "        preds.append(contentsOf: batchPreds)\n",
    "    }\n",
    "    return preds\n",
    "}\n",
    "\n",
    "let texts = [\n",
    "    \"A person is walking forwards.\", \n",
    "    \"A person walks 4 steps forward.\", \n",
    "    \"A person walks in a circle counter clockwise.\", \n",
    "    \"A person getting done on their knees\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do inference on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dsURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/labels_ds_v2_manual_labels.csv\")\n",
    "let df = pd.read_csv(dsURL.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 5 elements\n",
       "  - 0 : \"Doing something\"\n",
       "  - 1 : \"Walking and turning\"\n",
       "  - 2 : \"Walking backwards\"\n",
       "  - 3 : \"Walking few steps\"\n",
       "  - 4 : \"Walking or running\"\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let labels = df.label.unique().sorted().map {String($0)!}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3012\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let texts2: [String] = Array(df.text.to_list())! // .iloc[0..<2000]\n",
    "texts2.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doing something        1216\n",
       "Walking or running      649\n",
       "Walking and turning     644\n",
       "Walking few steps       400\n",
       "Walking backwards       103\n",
       "Name: label, dtype: int64\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict()\n",
      "texts: 3012\n",
      "validationExamples.count: 3012\n",
      "batchSize: 2048\n",
      "maxSequenceLength: 30\n",
      "batchSize / maxSequenceLength: 68\n",
      "validationBatches: 45\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n"
     ]
    }
   ],
   "source": [
    "let preds2 = predict(texts2, bertClassifier: bertClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for (idx, pred) in preds2.enumerated() {\n",
    "    print(idx, texts2[idx], pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## save preds"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df[\"pred\"] = PythonObject(preds2.map { $0.className })"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df[\"prob\"] = PythonObject(preds2.map { $0.probability })"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.to_csv(\"/notebooks/language2motion.gt/data/labels_ds_v1_preds.csv\", index: false)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## max seq len in tokens?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bertClassifier.bert.preprocess(sequences: [\"ala ma kota\"], maxSequenceLength: 200).tokenIds.shape[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let tokenCounts = texts2.map { bertClassifier.bert.preprocess(sequences: [$0], maxSequenceLength: 200).tokenIds.shape[1] }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenCounts.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
